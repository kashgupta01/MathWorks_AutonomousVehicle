{"cells":[{"cell_type":"markdown","metadata":{"id":"Zf8GI7grvFun"},"source":["# Initialize the Database"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ky_bc7jmxiYe"},"outputs":[],"source":["import os\n","import pathlib\n","import glob\n","import cv2 as cv2\n","import numpy as np\n","import xml.etree.ElementTree as ET\n","import pandas as pd\n","import tensorflow as tf\n","import skimage.io as io\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23550,"status":"ok","timestamp":1699361740004,"user":{"displayName":"Pamela Melgar","userId":"00048250327301414408"},"user_tz":300},"id":"GDv6tOEqxkOh","outputId":"fdb649b2-7c8a-4ac2-d618-a3a4fed3b864"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-11-07 11:55:15--  https://www.nuscenes.org/data/nuimages-v1.0-mini.tgz\n","Resolving www.nuscenes.org (www.nuscenes.org)... 13.227.219.45, 13.227.219.18, 13.227.219.6, ...\n","Connecting to www.nuscenes.org (www.nuscenes.org)|13.227.219.45|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 117929607 (112M) [application/x-tar]\n","Saving to: ‘nuimages-v1.0-mini.tgz’\n","\n","nuimages-v1.0-mini. 100%[===================>] 112.47M  26.1MB/s    in 4.7s    \n","\n","2023-11-07 11:55:20 (23.8 MB/s) - ‘nuimages-v1.0-mini.tgz’ saved [117929607/117929607]\n","\n"]}],"source":["!mkdir -p /data/sets/nuimages  # Make the directory to store the nuImages dataset in.\n","\n","!wget https://www.nuscenes.org/data/nuimages-v1.0-mini.tgz  # Download the nuImages mini split.\n","\n","!tar -xf nuimages-v1.0-mini.tgz -C /data/sets/nuimages  # Uncompress the nuImages mini split.\n","\n","!pip install nuscenes-devkit &> /dev/null  # Install nuImages."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1156,"status":"ok","timestamp":1699361765212,"user":{"displayName":"Pamela Melgar","userId":"00048250327301414408"},"user_tz":300},"id":"GFcbeisyxn9r","outputId":"27cd6ab6-6d5f-4bb4-ab1c-939b666144ea"},"outputs":[{"output_type":"stream","name":"stdout","text":["======\n","Loading nuImages tables for version v1.0-mini...\n","Done loading in 0.001 seconds (lazy=True).\n","======\n"]}],"source":["%matplotlib inline\n","%load_ext autoreload\n","%autoreload 2\n","from nuimages import NuImages\n","\n","nuim = NuImages(dataroot='/data/sets/nuimages', version='v1.0-mini', verbose=True, lazy=True)"]},{"cell_type":"markdown","metadata":{"id":"UxJ6USY4vFus"},"source":["Categories that are annotated"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":163,"status":"ok","timestamp":1699123592328,"user":{"displayName":"Kashish Gupta","userId":"03000924544158184884"},"user_tz":240},"id":"qH45VG4uvFus","outputId":"4ce844cf-9f4d-4dd9-e989-7505334e93a7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loaded 25 category(s) in 0.000s,\n","animal\n","flat.driveable_surface\n","human.pedestrian.adult\n","human.pedestrian.child\n","human.pedestrian.construction_worker\n","human.pedestrian.personal_mobility\n","human.pedestrian.police_officer\n","human.pedestrian.stroller\n","human.pedestrian.wheelchair\n","movable_object.barrier\n","movable_object.debris\n","movable_object.pushable_pullable\n","movable_object.trafficcone\n","static_object.bicycle_rack\n","vehicle.bicycle\n","vehicle.bus.bendy\n","vehicle.bus.rigid\n","vehicle.car\n","vehicle.construction\n","vehicle.ego\n","vehicle.emergency.ambulance\n","vehicle.emergency.police\n","vehicle.motorcycle\n","vehicle.trailer\n","vehicle.truck\n"]}],"source":["# The NuScenes class holds several tables. Each table is a list of records, and each record is a dictionary.\n","# For example the first record of the category table is stored at\n","\n","#nusc.category[0]['name']\n","\n","#these are the categories available\n","cat = []\n","for i in range(len(nuim.category)):\n","    print(nuim.category[i]['name'])\n","    cat.append(nuim.category[i]['name'])\n","\n"]},{"cell_type":"markdown","metadata":{"id":"jL8JYosavFut"},"source":["# classes that we are detecting :\n","\n","We merge adult, child, police officer, construction worker into a single class called pedestrian\n","We are detecting:\n","- pedestrian\n","- car\n","- bicycle"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SbFvRfl9vFut"},"outputs":[],"source":["classes = ['human.pedestrian.adult', 'human.pedestrian.child','human.pedestrian.police_officer','human.pedestrian.construction_worker','human.pedestrian.personal_mobility','human.pedestrian.stroller','human.pedestrian.wheelchair','vehicle.car','vehicle.bicycle']\n","pedestrians = ['human.pedestrian.adult', 'human.pedestrian.child','human.pedestrian.police_officer','human.pedestrian.construction_worker','human.pedestrian.personal_mobility','human.pedestrian.stroller','human.pedestrian.wheelchair']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":163,"status":"ok","timestamp":1699123596929,"user":{"displayName":"Kashish Gupta","userId":"03000924544158184884"},"user_tz":240},"id":"B7kfzUT9vFuu","outputId":"3424012b-b954-4d53-fddb-18b7f035f077"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total number of samples\n","Loaded 50 sample(s) in 0.000s,\n","50\n"]}],"source":["print('Total number of samples')\n","print(len(nuim.sample))\n","\n","total_no_of_samples = len(nuim.sample)\n","\n","#print('Total number of images')\n","#print(len(nusc.sample*6)) #6 different cameras"]},{"cell_type":"markdown","metadata":{"id":"SD3-YqvJvFuu"},"source":["# Functions"]},{"cell_type":"markdown","metadata":{"id":"Pbd62k2MvFuv"},"source":["Defined the following function:\n","\n","- get_sample_data (edit of nutonomy's original nusc.get_sample_data)\n","        \n","        input:(nusc, sample_data_token)\n","        output:path to the data, lists of 3d bounding boxes in the image (in camera coordinates),\n","        annotation token of annotations in the image, intrinsic matrix of the camera)\n","        \n","\n","- threeD_to_2D\n","         \n","        input: (box (camera coordinates),intrinsic matrix))\n","        output : corners of the 2d bounding box in image plane\n","\n","- all_3d_to_2d(boxes,anns,intrinsic)\n","\n","        input : boxes in camera coordinates, list of annotation tokens of annotations in the image,\n","        intrinsic matrix\n","        output: x_min,x_max,y_min,y_max,width,height of the 2D boundings boxes of objects that are\n","        more than 40% visible in panoramic view of all cameras, also ensures that the center of the\n","        bounding boxes falls inside the image\n","\n","- extract_bounding_box(i):\n","        \n","        input: sample number\n","        output: min x, max x, min y max y, width and height of bounding box in image coordinates\n","        2d bounding box of objects which are 40% visible in panoramic view of all cameras and center\n","        falls witin the image\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5_0zcBf5vFuv"},"outputs":[],"source":["#from pyquaternion import Quaternion\n","#from nuscenes.utils.data_classes import Box\n","#from nuscenes.utils.geometry_utils import quaternion_slerp, box_in_image, BoxVisibility\n","import numpy as np\n","def get_sample_data(nuim, sample_data_token, selected_anntokens=None):\n","    \"\"\"\n","    Returns the data path as well as all annotations related to that sample_data(single image).\n","    Note that the boxes are transformed into the current sensor's coordinate frame.\n","    :param sample_data_token: <str>. Sample_data token(image token).\n","    :param box_vis_level: <BoxVisibility>. If sample_data is an image, this sets required visibility for boxes.\n","    :param selected_anntokens: [<str>]. If provided only return the selected annotation.\n","    :return: (data_path <str>, boxes [<Box>], camera_intrinsic <np.array: 3, 3>)\n","    \"\"\"\n","\n","    # Retrieve sensor & pose records\n","    sd_record = nuim.get('sample_data', sample_data_token)\n","    cs_record = nuim.get('calibrated_sensor', sd_record['calibrated_sensor_token'])\n","    sensor_record = nuim.get('sensor', cs_record['sensor_token'])\n","    pose_record = nuim.get('ego_pose', sd_record['ego_pose_token'])\n","\n","    sample_record = nuim.get('sample',sd_record['sample_token'])\n","    data_path = nuim.get_sample_data_path(sample_data_token)\n","\n","    if sensor_record['modality'] == 'camera':\n","        cam_intrinsic = np.array(cs_record['camera_intrinsic'])\n","        imsize = (sd_record['width'], sd_record['height'])\n","    else:\n","        cam_intrinsic = None\n","        imsize = None\n","\n","    # Retrieve all sample annotations and map to sensor coordinate system.\n","    if selected_anntokens is not None:\n","        boxes = list(map(nuim.get_box, selected_anntokens))\n","    else:\n","        boxes = nuim.get_boxes(sample_data_token)\n","        selected_anntokens = sample_record['anns']\n","\n","    # Make list of Box objects including coord system transforms.\n","    box_list = []\n","    ann_list = []\n","    for box,ann in zip(boxes,selected_anntokens):\n","\n","        # Move box to ego vehicle coord system\n","        box.translate(-np.array(pose_record['translation']))\n","        box.rotate(Quaternion(pose_record['rotation']).inverse)\n","\n","        #  Move box to sensor coord system\n","        box.translate(-np.array(cs_record['translation']))\n","        box.rotate(Quaternion(cs_record['rotation']).inverse)\n","\n","        if sensor_record['modality'] == 'camera' and not \\\n","                box_in_image(box, cam_intrinsic, imsize, vis_level=box_vis_level):\n","            continue\n","\n","        box_list.append(box)\n","        ann_list.append(ann)\n","    #this is for a single sample image\n","    return data_path, box_list, ann_list, cam_intrinsic #single image info"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6DpHquKnvFuw"},"outputs":[],"source":["def threeD_2_twoD(boxsy,intrinsic): #input is a single annotation box\n","    '''\n","    given annotation boxes and intrinsic camera matrix\n","    outputs the 2d bounding box coordinates as a list (all annotations for a particular sample image)\n","    '''\n","    corners = boxsy.corners()\n","    x = corners[0,:]\n","    y = corners[1,:]\n","    z = corners[2,:]\n","    x_y_z = np.array((x,y,z))\n","    orthographic = np.dot(intrinsic,x_y_z)\n","    perspective_x = orthographic[0]/orthographic[2]\n","    perspective_y = orthographic[1]/orthographic[2]\n","    perspective_z = orthographic[2]/orthographic[2]\n","\n","    min_x = np.min(perspective_x)\n","    max_x = np.max(perspective_x)\n","    min_y = np.min(perspective_y)\n","    max_y = np.max(perspective_y)\n","\n","\n","\n","    return min_x,max_x,min_y,max_y\n","\n","\n","\n","def all_3d_to_2d(boxes,anns,intrinsic): #input 3d boxes, annotation key lists, intrinsic matrix (one image)\n","    x_min=[]\n","    x_max=[]\n","    y_min=[]\n","    y_max =[]\n","    width=[]\n","    height=[]\n","    objects_detected =[]\n","    orig_objects_detected =[]\n","\n","\n","    for j in range(len(boxes)): #iterate through boxes\n","        box=boxes[j]\n","\n","        if box.name in classes: #if the box.name is in the classes we want to detect\n","\n","            if box.name in pedestrians:\n","                orig_objects_detected.append(\"pedestrian\")\n","            elif box.name == \"vehicle.car\":\n","                orig_objects_detected.append(\"car\")\n","            else:\n","                orig_objects_detected.append(\"cyclist\")\n","            #print(box)\n","\n","            visibility = nusc.get('sample_annotation', '%s' %anns[j])['visibility_token'] #give annotation key\n","            visibility = int(visibility)\n","\n","\n","            if visibility > 1: #more than 40% visible in the panoramic view of the the cameras\n","\n","\n","                center = box.center #get boxe's center\n","\n","                center = np.dot(intrinsic,center)\n","                center_point = center/(center[2]) #convert center point into image plane\n","\n","\n","\n","\n","                if center_point[0] <-100 or center_point[0] > 1700 or center_point[1] <-100 or center_point[1] >1000:\n","                    #if center of bounding box is outside of the image, do not annotate\n","                    pass\n","\n","                else:\n","                    min_x, max_x, min_y, max_y = threeD_2_twoD(box,intrinsic) #converts box into image plane\n","                    w = max_x - min_x\n","                    h = max_y - min_y\n","\n","\n","                    x_min.append(min_x)\n","                    x_max.append(max_x)\n","                    y_min.append(min_y)\n","                    y_max.append(max_y)\n","                    width.append(w)\n","                    height.append(h)\n","                    if box.name in pedestrians:\n","                        objects_detected.append(\"pedestrian\")\n","                    elif box.name == \"vehicle.car\":\n","                        objects_detected.append(\"car\")\n","                    else:\n","                        objects_detected.append(\"cyclist\")\n","\n","\n","            else:\n","                pass\n","\n","    return x_min,x_max,y_min,y_max,width,height,objects_detected,orig_objects_detected #for a single image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QLAnAawVvFuw"},"outputs":[],"source":["def extract_bounding_box(i,camera_name): #give a single sample number and camera name\n","\n","    '''\n","    input sample number i, camera name\n","    outputs min x, max x, min y max y, width and height of bounding box in image coordinates\n","    2d bounding box\n","    options for camera name : CAM_FRONT, CAM_FRONT_RIGHT, CAM_FRONT_LEFT, CAM_BACK, CAM_BACK_RIGHT,CAM_BACK_LEFT\n","    '''\n","\n","    nuim.sample[i] #one image\n","\n","    camera_token = nuim.sample[i]['data']['%s' %camera_name] #one camera, get the camera token\n","\n","    path, boxes, anns, intrinsic_matrix = get_sample_data(nuim,'%s' %camera_token) #gets data for one image\n","\n","    x_min, x_max,y_min,y_max,width,height, objects_detected,orig_objects_detected = all_3d_to_2d(boxes,anns, intrinsic_matrix)\n","\n","    return x_min, x_max, y_min, y_max, width, height, path, boxes,intrinsic_matrix, objects_detected,orig_objects_detected\n","    #info for a single image\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sSKAdLBrvFux"},"outputs":[],"source":["#Create target Directory if don't exist\n","import os.path\n","def create_annotation_directory(camera):\n","    current_dir =os.getcwd()\n","    #current_dir =\"%s/annotation\" %pwd\n","    dirName =\"%s/annotation/%s_anno\" %(current_dir,camera)\n","    if not os.path.exists(dirName):\n","        os.makedirs(dirName)\n","        print(\"Directory \" , dirName ,  \" Created \")\n","    else:\n","        print(\"Directory \" , dirName ,  \" already exists\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"olM4LFJNvFux"},"outputs":[],"source":["from lxml import etree as ET\n","def write_xml_annotation(x_min,x_max,y_min,y_max,width,height,path,boxes,objects_detected): #single image info\n","    #detected_items =[]\n","    #import xml.etree.cElementTree as ET\n","    path_split = path.split(\"/\")\n","    full_image_name = path_split[-1]\n","    name =full_image_name.split(\".\")[0]\n","\n","    root = ET.Element(\"annotation\")\n","\n","\n","    ET.SubElement(root, \"folder\").text = \"%s\" %camera\n","    ET.SubElement(root, \"filename\").text = \"%s\" %full_image_name\n","    ET.SubElement(root, \"path\").text = \"%s\" %path\n","\n","    source = ET.SubElement(root, \"source\")\n","    ET.SubElement(source, \"database\").text = \"nuTonomy-nuscenes\"\n","\n","    size = ET.SubElement(root, \"size\")\n","    ET.SubElement(size, \"width\").text=\"1600\"\n","    ET.SubElement(size,\"height\").text=\"900\"\n","    ET.SubElement(size,\"depth\").text=\"3\"\n","    ET.SubElement(root, \"segmented\").text = \"0\"\n","\n","    for j in range(len(objects_detected)): #\n","\n","        flag_x = 0\n","        flag_y = 0\n","\n","        ob= ET.SubElement(root, \"object\")\n","        ET.SubElement(ob,\"name\").text=\"%s\" %objects_detected[j]\n","        ET.SubElement(ob,\"pose\").text=\"Unspecified\"\n","\n","\n","        '''\n","        write out truncated boxes\n","        '''\n","\n","        if x_min[j] < 0:\n","            x_minsy = 0\n","            flag_x =1\n","\n","        else:\n","            x_minsy = x_min[j]\n","\n","        if y_min[j] <0:\n","            y_minsy = 0\n","            flag_y =1\n","\n","        else:\n","            y_minsy = y_min[j]\n","\n","        if x_max[j] > 1600:\n","            x_maxsy = 1600\n","            flag_x = 1\n","\n","        else:\n","            x_maxsy = x_max[j]\n","\n","        if y_max[j] >900:\n","            y_maxsy = 900\n","            flag_y = 1\n","\n","        else:\n","            y_maxsy = y_max[j]\n","\n","\n","        if flag_x == 1 or flag_y ==1:\n","            ET.SubElement(ob, \"truncated\").text=\"1\"\n","\n","        else:\n","            ET.SubElement(ob, \"truncated\").text=\"0\"\n","\n","\n","\n","\n","        ET.SubElement(ob, \"difficult\").text=\"0\"\n","\n","        bb = ET.SubElement(ob,\"bndbox\")\n","\n","\n","        ET.SubElement(bb,\"xmin\").text=\"%s\" %x_minsy\n","        ET.SubElement(bb,\"ymin\").text=\"%s\" %y_minsy\n","        ET.SubElement(bb,\"xmax\").text=\"%s\" %x_maxsy\n","        ET.SubElement(bb,\"ymax\").text=\"%s\" %y_maxsy\n","\n","\n","    filename = \"%s/%s.xml\" %(dirName,name)\n","    tree = ET.ElementTree(root)\n","    #tree.write(\"%s/%s.xml\" %(dirName,name),pretty_print=True)\n","    tree.write(\"%s\" %filename, pretty_print=True)\n","\n","    return filename #file a single file\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":507},"executionInfo":{"elapsed":139,"status":"error","timestamp":1699124592135,"user":{"displayName":"Kashish Gupta","userId":"03000924544158184884"},"user_tz":240},"id":"tbTNQqp9vFux","outputId":"1af873b4-9577-4701-e422-626603a6f38e"},"outputs":[{"name":"stdout","output_type":"stream","text":["CAM_FRONT\n","Directory  /content/annotation/CAM_FRONT_anno  Created \n"]},{"ename":"KeyError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-dca7a1d82701>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mx_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_max\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_min\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_max\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintrinsic_matrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mobjects_detected\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0morig_objects_detected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_bounding_box\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%s'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0mcamera\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mwrite_xml_annotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_min\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_max\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_min\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_max\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mobjects_detected\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-12-5cb7eebbf418>\u001b[0m in \u001b[0;36mextract_bounding_box\u001b[0;34m(i, camera_name)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mnuim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#one image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mcamera_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnuim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'%s'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0mcamera_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#one camera, get the camera token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintrinsic_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_sample_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnuim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'%s'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0mcamera_token\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#gets data for one image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'data'"]}],"source":["camera_names =['CAM_FRONT', 'CAM_FRONT_RIGHT', 'CAM_FRONT_LEFT', 'CAM_BACK', 'CAM_BACK_RIGHT', 'CAM_BACK_LEFT']\n","\n","i = 0\n","detected_items =[]\n","orig_detected_items=[]\n","obs = []\n","\n","file=[]\n","\n","for camera in camera_names: #iterate through all cameras\n","    print(camera)\n","    create_annotation_directory(camera)\n","    current_dir =os.getcwd()\n","    dirName =\"%s/annotation/%s_anno\" %(current_dir,camera) #current directory's name\n","    #we are looking at one camera now\n","    for sample_number in range(total_no_of_samples):#look at a single image\n","        #print(sample_number)\n","        #get in for a single image\n","\n","\n","\n","\n","        x_min, x_max,y_min,y_max,width,height, path, boxes, intrinsic_matrix,objects_detected,orig_objects_detected = extract_bounding_box(sample_number, '%s' %camera)\n","        write_xml_annotation(x_min,x_max,y_min,y_max,width,height,path,boxes,objects_detected)\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cGwkczktvFuy"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zQDSmN74vFuy"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gvennYrSvFuy"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p-c1MuYevFuy"},"outputs":[],"source":["print(len(obs))\n","print(len(orig_detected_items))\n","\n","print(len(file))\n","\n","unique = list(set(file))\n","print(len(unique))\n","\n","print('total number of files')\n","3962*6"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E2Fm5KPOvFuy"},"outputs":[],"source":["print(len(file))\n","#print(len(unique))\n","\n","for i in range(len(file)):\n","    check = file[i]\n","\n","    for j in range(len(file)):\n","        if j !=i :\n","            if check == file[j]:\n","                print(i)\n","                print(j)\n","                print('katie')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XLFDVYmGvFuy"},"outputs":[],"source":["import os.path\n","from os import listdir\n","import xml.etree.ElementTree as ET\n","camera_names =['CAM_FRONT', 'CAM_FRONT_RIGHT', 'CAM_FRONT_LEFT', 'CAM_BACK', 'CAM_BACK_RIGHT', 'CAM_BACK_LEFT']\n","\n","def list_of_files(camera):\n","    current_dir =os.getcwd()\n","    #current_dir =\"%s/annotation\" %pwd\n","\n","    dirName =\"%s/annotation/%s_anno\" %(current_dir,camera)\n","    files = os.listdir(dirName)\n","\n","    return files, dirName, current_dir"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CZB2wqyWvFuz"},"outputs":[],"source":["total_objects_detected =[]\n","for camera in camera_names:\n","    files, dirName,current_dir = list_of_files(camera)\n","    print(dirName)\n","\n","    for f in files:\n","        name_of_file = '%s/%s' %(dirName, f)\n","        #print(name_of_file)\n","        w,h,od = extract_data(name_of_file,dirName)\n","        total_objects_detected = total_objects_detected + od\n","        #print(od)\n","        #print(od)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TExmlvL7vFuz"},"outputs":[],"source":["print(len(total_objects_detected))\n","print(len(detected_items))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QGRJu03WvFuz"},"outputs":[],"source":["print(orig_detected_items.count('car'))\n","print(orig_detected_items.count('pedestrian'))\n","print(orig_detected_items.count('cyclist'))\n","\n","\n","add = orig_detected_items.count('car') + orig_detected_items.count('pedestrian') + orig_detected_items.count('cyclist')\n","print(add)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KuCmV7bLvFuz"},"outputs":[],"source":["import os\n","print(os.getcwd())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PuCrQzwJvFuz"},"outputs":[],"source":["#print(len(detected_items))\n","print('Total number of car annotations:')\n","print(detected_items.count('car'))\n","print('Total number of pedestrian annotations')\n","print(detected_items.count('pedestrian'))\n","print('Total number of cyclist annotations')\n","print(detected_items.count('cyclist'))\n","\n","add = detected_items.count('car') + detected_items.count('pedestrian') + detected_items.count('cyclist')\n","print(add)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XPyoyCcKvFu0"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","from PIL import Image\n","import numpy as np\n","\n","im = np.array(Image.open('/Volumes/Luthor/nutonomy/nuscenes-devkit-master/python-sdk/%s' %path), dtype=np.uint8)\n","\n","fig = plt.figure(figsize=(10,10))\n","ax = fig.add_subplot(1, 1, 1)\n","# Create figure and axes\n","\n","\n","# Display the image\n","ax.imshow(im)\n","#\n","\n","#print(x)\n","#print(y)\n","#print(width)\n","#print(height)\n","#print(center_point[0])\n","#print(center_point[1])\n","\n","#width = max_x-min_x\n","#height = max_y-min_y\n","#for i in range(len(perspective_x)):\n","#ax.plot(center_point[0], center_point[1], marker ='o', color='b', markersize =30)\n","#ax.plot(perspective_x[i], perspective_y[i], marker ='o', color='b', markersize =10)\n","#ax.plot(r2c2[0], r2c2[1], marker ='o', color='b', markersize =10)\n","#ax.plot(min_x, min_y, marker ='o', color='b', markersize =10)\n","#ax.plot(max_x, max_y, marker ='o', color='b', markersize =10)\n","\n","for i in range(len(x_min)):\n","    if objects_detected[i] =='pedestrian':\n","        col = 'red'\n","    elif objects_detected[i] =='car':\n","        col ='green'\n","    else:\n","        col= 'blue'\n","    rect = patches.Rectangle((x_min[i],y_min[i]),width[i],height[i],linewidth=2,edgecolor='%s' %col,facecolor='none')\n","#ax.plot(k3[0], k3[1], marker ='o', color='b', markersize =10)\n","#ax.plot(k4[0], k4[1], marker ='o', color='b', markersize =10)\n","#ax.plot(k5[0], k5[1], marker ='o', color='b', markersize =10)\n","#ax.plot(k6[0], k6[1], marker ='o', color='b', markersize =10)\n","#ax.plot(k7[0], k7[1], marker ='o', color='b', markersize =10)\n","#ax.plot(k8[0], k8[1], marker ='o', color='b', markersize =10)\n","\n","#rect = patches.Rectangle((x,y),width,height,linewidth=1,edgecolor='blue',facecolor='none')\n","\n","# Add the patch to the Axes\n","    ax.add_patch(rect)\n","plt.savefig('foo.jpeg')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xh88dyrQvFu0"},"outputs":[],"source":["#3d render with original bounding boxes\n","#343\n","#369\n","\n","#383\n","#357\n","sample_number =348\n","camera = 'CAM_FRONT_RIGHT'\n","my_sample = nusc.sample[sample_number]\n","nusc.render_sample_data(my_sample['data']['%s' %camera])\n","print(my_sample)\n","print('this is the path')\n","\n","nusc.get('sample_data', 'bde261e2ea904fcd86cef6e007bdfdb4')\n","\n","\n","f1= 'samples/CAM_FRONT_RIGHT/n008-2018-05-21-11-06-59-0400__CAM_FRONT_RIGHT__1526915624869956.jpg'\n","f2 ='samples/CAM_FRONT_RIGHT/n008-2018-05-21-11-06-59-0400__CAM_FRONT_RIGHT__1526915624869956.jpg'\n","\n","if f1 ==f2:\n","    print('katie')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NIzM3JS1vFu1"},"outputs":[],"source":["sample_number =374\n","camera = 'CAM_FRONT_RIGHT'\n","my_sample = nusc.sample[sample_number]\n","nusc.render_sample_data(my_sample['data']['%s' %camera])\n","print(my_sample)\n","\n","#'5eedbe17cf2f44e2829567eeeb12f569'\n","\n","print('this is the path')\n","\n","nusc.get('sample_data', '2cab2f94315e47eea4e4409d7906db6b')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TueAJk6JvFu1"},"outputs":[],"source":["#import xml.etree.cElementTree as ET\n","from lxml import etree as ET\n","root = ET.Element(\"annotation\")\n","\n","\n","ET.SubElement(root, \"folder\").text = \"captures_vlc\"\n","ET.SubElement(root, \"filename\").text = \"katie.jpg\"\n","ET.SubElement(root, \"path\").text = \"katie.jpg\"\n","\n","source = ET.SubElement(root, \"source\")\n","ET.SubElement(source, \"database\").text = \"nuTonomy-nuscenes\"\n","\n","size = ET.SubElement(root, \"size\")\n","ET.SubElement(size, \"width\").text=\"Katie\"\n","ET.SubElement(size,\"height\").text=\"Kates\"\n","ET.SubElement(size,\"depth\").text=\"KM\"\n","ET.SubElement(root, \"segmented\").text = \"0\"\n","\n","ob= ET.SubElement(root, \"object\")\n","ET.SubElement(ob,\"name\").text=\"ball\"\n","ET.SubElement(ob,\"pose\").text=\"Unspecified\"\n","ET.SubElement(ob, \"truncated\").text=\"truncated\"\n","ET.SubElement(ob, \"difficult\").text=\"0\"\n","\n","bb = ET.SubElement(ob,\"bndbox\")\n","ET.SubElement(bb,\"xmin\").text=\"xmin\"\n","ET.SubElement(bb,\"ymin\").text=\"ymin\"\n","ET.SubElement(bb,\"xmax\").text=\"xmax\"\n","ET.SubElement(bb,\"ymax\").text=\"ymax\"\n","\n","\n","tree = ET.ElementTree(root)\n","tree.write(\"%s.xml\" %name,pretty_print=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZqkwHfhivFu1"},"outputs":[],"source":["i = 0\n","with open('images_with_no_annotations.txt') as f:\n","    for line in f:\n","        #print(line)\n","\n","        i = i +1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IkN1D-movFu1"},"outputs":[],"source":["print(i)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J-qlp6lnvFu2"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/asvath/mobile_robotics/blob/master/nuscenes%20extract%20and%20write%20out%202d%20annotation%20boxes-revised%20to%20truncate%20bb.ipynb","timestamp":1699058241581}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":0}